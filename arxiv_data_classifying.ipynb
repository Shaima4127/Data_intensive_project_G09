{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This file is used for classifying all the articles and to see the result before we create arxiv_classifying.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, split, when, count\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from delta import DeltaTable, configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_ML_pipline(ML_model = \"NB\"):\n",
    "  # Convert the main_category column to numeric using StringIndexer\n",
    "  labelIndexer = StringIndexer(inputCol=\"main_category\", outputCol=\"label\")\n",
    "\n",
    "  # Define the regular expression tokenizer\n",
    "  regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "  # Define the stop words remover\n",
    "  stopWordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "  # Define the TF-IDF Vectorizer\n",
    "  countVectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"vectorize_features\")\n",
    "  idf = IDF(inputCol=\"vectorize_features\", outputCol=\"features\")\n",
    "\n",
    "  if ML_model == 'LR': # Create logistic regression classifier     \n",
    "    ML_Model = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "  elif ML_model == 'RF': # Create a Random Forest classifier\n",
    "    ML_Model = RandomForestClassifier(numTrees=100, maxDepth=5, labelCol=\"label\", featuresCol=\"features\")\n",
    "  elif ML_model == 'NB': # Create a Naive Bayes classifier\n",
    "    ML_Model = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "  # Define the Pipeline\n",
    "  pipeline = Pipeline(stages=[labelIndexer, regexTokenizer, stopWordsRemover, countVectorizer, idf, ML_Model])\n",
    "\n",
    "  return pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c0e8b289-2d7c-4460-b5e9-841cf4c7f429;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 226ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c0e8b289-2d7c-4460-b5e9-841cf4c7f429\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:33:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:33:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/04/26 12:34:25 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 12:34:25 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar added multiple times to distributed cache.\n",
      "23/04/26 12:34:25 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "# the final result for prediction will be saved in the path as a Delta table\n",
    "delta_table_path = \"hdfs:///Dat500_Group09/spark_result/final_result/arxiv_meta\" \n",
    "\n",
    "# Set the configuration properties for Delta tables\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Arxiv_Classification_baseline\") \\\n",
    "    .master('yarn') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 24)\\\n",
    "    .config('spark.executor.instances', 8)\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\") \n",
    "        #.config('spark.driver.memory', \"8g\")\\ take the default from the configuration file we set it 8gb\n",
    "\n",
    "    \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.executor.memory: 2g\n",
      "spark.yarn.dist.jars: file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar\n",
      "spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite: true\n",
      "spark.sql.shuffle.partitions: 24\n",
      "spark.submit.pyFiles: /home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar,/home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar,/home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar\n",
      "spark.databricks.delta.properties.defaults.autoOptimize.autoCompact: true\n",
      "spark.executor.instances: 8\n",
      "spark.jars.packages: io.delta:delta-core_2.12:2.3.0\n",
      "spark.master: yarn\n",
      "spark.app.name: Arxiv_Classification_baseline\n",
      "spark.ui.proxyBase: /proxy/application_1679580022279_0242\n",
      "spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension\n",
      "spark.driver.memory: 8g\n",
      "spark.yarn.isPython: true\n",
      "spark.submit.deployMode: client\n",
      "spark.yarn.dist.pyFiles: file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar\n",
      "spark.repl.local.jars: file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.3.0.jar,file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar\n",
      "spark.app.submitTime: 1682512419175\n",
      "spark.ui.showConsoleProgress: true\n",
      "spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "config_map = conf.getAll()\n",
    "for key, value in config_map:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:39:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Adaptive Query Execution is enabled\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "aqe_enabled = spark.conf.get(\"spark.sql.adaptive.enabled\")\n",
    "print(f\"Adaptive Query Execution is {'enabled' if aqe_enabled == 'true' else 'disabled'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:40:26 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "|       id|                text|               title|            abstract|authers_num|article_date|main_category|          main_topic|\n",
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "|0704.0003|The evolution of ...|The evolution of ...| The evolution of...|          1|  2007-04-01|      Physics|     General Physics|\n",
      "|0704.0004|A determinant of ...|A determinant of ...| We show that a d...|          1|  2007-04-01|  Mathematics|       Combinatorics|\n",
      "|0704.0005|From dyadic $\\\\La...|From dyadic $\\\\La...| In this paper we...|          2|  2007-04-01|  Mathematics|Classical Analysi...|\n",
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading from delta table Training & Testing data\n",
    "delta_training_file = \"hdfs:///Dat500_Group09/spark_result/training\"\n",
    "delta_testing_file = \"hdfs:///Dat500_Group09/spark_result/testing/test1\"\n",
    "\n",
    "trainingData = spark.read.format(\"delta\").load(delta_training_file)\n",
    "    \n",
    "trainingData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "|       id|                text|               title|            abstract|authers_num|article_date|main_category|          main_topic|\n",
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "|1003.2608|Dissipative Trans...|Dissipative Trans...| After almost hal...|          4|  2010-03-01|      Physics|       Quantum Gases|\n",
      "|1003.2609|Spin dynamics in ...|Spin dynamics in ...| The doping depen...|          5|  2010-03-01|      Physics|   Superconductivity|\n",
      "|1003.2610|Carbon fibre tips...|Carbon fibre tips...| We report the fa...|          2|  2010-03-01|      Physics|Mesoscale and Nan...|\n",
      "+---------+--------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testData = spark.read.format(\"delta\").load(delta_testing_file)\n",
    "testData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Create_ML_pipline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size:  1672474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===============================================>        (42 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data size:  1043714\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training Data size: \", trainingData.count())\n",
    "print(\"Testing Data size: \", testData.count())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:46:07 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:48:41 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:48:45 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:50:19 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "ML_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 04:13:20 WARN TaskSetManager: Stage 229 contains a task of very large size (4849 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 04:13:22 WARN TaskSetManager: Stage 233 contains a task of very large size (4184 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/21 04:13:23 WARN TaskSetManager: Stage 237 contains a task of very large size (16725 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# future improvement\n",
    "# save the machine learning model\n",
    "#pipelinePath = 'hdfs:///Dat500_Group09/result/ML_model'\n",
    "#ML_model.write().overwrite().save(pipelinePath)\n",
    "\n",
    "\n",
    "# load the saved machine learning model\n",
    "#from pyspark.ml import PipelineModel\n",
    "#savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "df_Prediction = ML_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:52:01 WARN DAGScheduler: Broadcasting large task binary with size 22.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----+----------+\n",
      "|        id|   main_category|label|prediction|\n",
      "+----------+----------------+-----+----------+\n",
      "|2205.05485|     Mathematics|  1.0|       1.0|\n",
      "|2205.05487|Computer Science|  2.0|       2.0|\n",
      "|2205.05488|Computer Science|  2.0|       5.0|\n",
      "+----------+----------------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the dataframe with the original main_category and the predicted one\n",
    "df_Prediction = df_Prediction.select(\"id\", \"main_category\", \"label\", \"prediction\")\n",
    "df_Prediction.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:52:23 WARN DAGScheduler: Broadcasting large task binary with size 22.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.850805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(df_Prediction)\n",
    "print(\"accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update delta table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:54:03 WARN DAGScheduler: Broadcasting large task binary with size 22.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 12:55:15 WARN DAGScheduler: Broadcasting large task binary with size 22.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check if the Delta table exists  \n",
    "# DeltaTable.isDeltaTable(spark, \"spark-warehouse/table1\") # True \n",
    "#DeltaTable\n",
    "if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "    print(\"update delta table\")\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "    #\"target.id = updates.id and target.main_category = updates.main_category\") \\\n",
    "    deltaTable.alias(\"target\") \\\n",
    "        .merge(\n",
    "        source = df_Prediction.alias(\"updates\"),\n",
    "        condition = \"target.id = updates.id\") \\\n",
    "        .whenMatchedUpdate( set = \n",
    "        {\n",
    "            \"label\": \"updates.label\",\n",
    "            \"prediction\": \"updates.prediction\"     \n",
    "        }) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "        {\n",
    "            \"id\": \"updates.id\",\n",
    "            \"main_category\": \"updates.main_category\",\n",
    "            \"label\": \"updates.label\",\n",
    "            \"prediction\": \"updates.prediction\"        \n",
    "        }) \\\n",
    "        .execute()\n",
    "else: # file not exists\n",
    "    print(\"Create delta table first time\")\n",
    "    df_Prediction.write.format(\"delta\").save(delta_table_path)\n",
    "    #df_Prediction.write.format(\"delta\").partitionBy(\"main_category\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:====================================================>   (47 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----+----------+\n",
      "|       id|main_category|label|prediction|\n",
      "+---------+-------------+-----+----------+\n",
      "|0704.0011|  Mathematics|  1.0|       1.0|\n",
      "|0704.0033|      Physics|  0.0|       3.0|\n",
      "|0704.0037|      Physics|  0.0|       0.0|\n",
      "|0704.0072|      Physics|  0.0|       1.0|\n",
      "|0704.0079|  Mathematics|  1.0|       1.0|\n",
      "|0704.0107|      Physics|  0.0|       3.0|\n",
      "|0704.0111|  Mathematics|  1.0|       1.0|\n",
      "|0704.0173|      Physics|  0.0|       0.0|\n",
      "|0704.0185|      Physics|  0.0|       0.0|\n",
      "|0704.0194|      Physics|  0.0|       0.0|\n",
      "+---------+-------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1043714"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
